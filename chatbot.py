import streamlit as st
import os
from google.oauth2 import service_account

# Gerekli LangChain importlarÄ±
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# -------------------------------
# 1. Service Account ve Ortam AyarlarÄ±
# -------------------------------
SERVICE_ACCOUNT_FILE = r"C:\Users\HanÄ±m\Desktop\GenAIMuzikAsistani\service-account.json"

if not os.path.exists(SERVICE_ACCOUNT_FILE):
    st.error(f"Hata: Service Account JSON dosyasÄ± bulunamadÄ±. LÃ¼tfen '{SERVICE_ACCOUNT_FILE}' dosyasÄ±nÄ± kontrol edin.")
    st.stop()

# Credentials oluÅŸtur
credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)

# -------------------------------
# 2. RAG Pipeline Kurulumu
# -------------------------------
@st.cache_resource
def setup_rag_pipeline(pdf_path):
    # Streamlit, spinner iÃ§eriÄŸini otomatik olarak gÃ¶sterir.
    with st.spinner("MÃ¼zik teorisi belgeleri yÃ¼kleniyor ve vektÃ¶r veritabanÄ± hazÄ±rlanÄ±yor..."):
        if not os.path.exists(pdf_path):
            st.error(f"Hata: Veri dosyasÄ± bulunamadÄ±. LÃ¼tfen '{pdf_path}' dosyasÄ±nÄ± proje klasÃ¶rÃ¼ne koyun.")
            return None

        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # Chunking (ParÃ§alama)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)

        # Embedding ve VektÃ¶r VeritabanÄ±
        # Yeni hata "unexpected model name format" nedeniyle,
        # model adÄ±nÄ± tam API yoluyla ('models/') gÃ¼ncelliyoruz.
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004", 
            credentials=credentials
        )

        try:
            vectordb = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
            retriever = vectordb.as_retriever(search_kwargs={"k": 3})
            return retriever
        except Exception as e:
            st.error(f"VektÃ¶r veritabanÄ± oluÅŸturulurken bir hata oluÅŸtu: {e}")
            return None


# -------------------------------
# 3. Chatbot Fonksiyonu
# -------------------------------
def main():
    st.set_page_config(page_title="ğŸ¼ MÃ¼zik NotalarÄ± RAG Chatbotu", layout="wide")
    st.title("ğŸµ MÃ¼zik NotalarÄ± AsistanÄ± (RAG Destekli)")

    PDF_DOSYA_ADI = "Temel MÃ¼zik EÄŸitimi.pdf"
    retriever = setup_rag_pipeline(PDF_DOSYA_ADI)
    if retriever is None:
        return

    # LLM (Google Gemini)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.2,
        credentials=credentials
    )

    # Prompt TanÄ±mÄ±
    prompt = ChatPromptTemplate.from_template("""
    AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak soruyu yanÄ±tla. YanÄ±t TÃ¼rkÃ§e, kÄ±sa ve doÄŸru olmalÄ±. 
    EÄŸer baÄŸlamda soruya cevap verecek yeterli bilgi yoksa, "Elimdeki mÃ¼zik teorisi bilgileri bu soruyu tam olarak yanÄ±tlamak iÃ§in yetersizdir." de.

    BaÄŸlam:
    {context}

    Soru:
    {input}
    """)

    # Stuff Chain
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    
    # RAG zinciri
    qa_chain = create_retrieval_chain(retriever, combine_docs_chain)

    # Streamlit oturum durumu baÅŸlatma
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # MesajlarÄ± gÃ¶ster
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ± giriÅŸi
    if user_query := st.chat_input("ğŸ¤ MÃ¼zikle ilgili bir soru sor..."):
        # KullanÄ±cÄ± mesajÄ±nÄ± ekle
        st.session_state["messages"].append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        # CevabÄ± al ve gÃ¶ster
        with st.chat_message("assistant"):
            with st.spinner("Cevap aranÄ±yor ve oluÅŸturuluyor..."):
                try:
                    response = qa_chain.invoke({"input": user_query})
                    answer = response["answer"]
                    
                    # KaynaklarÄ± cevaba ekle
                    sources_info = "\n\n**ğŸ“š Kaynaklar:**\n"
                    for i, doc in enumerate(response["context"]):
                        src = doc.metadata.get("source", "Bilinmiyor")
                        page = doc.metadata.get("page", "Bilinmiyor")
                        sources_info += f"- Sayfa {page}: {src.split('/')[-1]}\n"
                    
                    full_response = answer + sources_info
                    
                    st.markdown(answer)
                    st.markdown(sources_info)
                    
                    # Asistan mesajÄ±nÄ± oturum durumuna ekle
                    st.session_state["messages"].append({"role": "assistant", "content": full_response})

                except Exception as e:
                    error_message = f"Cevap oluÅŸturulurken bir hata oluÅŸtu: {e}"
                    st.error(error_message)
                    st.session_state["messages"].append({"role": "assistant", "content": error_message})


# -------------------------------
# 4. Uygulama BaÅŸlat
# -------------------------------
if __name__ == "__main__":
    main()
